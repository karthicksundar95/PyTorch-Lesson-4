{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lesson 3\n",
    "\n",
    "\n",
    "Things we will learn in this lesson:\n",
    "\n",
    "    - Multiple Linear Regression\n",
    "    - how to run and predict the output\n",
    "    - Logistic regression basics\n",
    "    - Run logistic regression in pytorch for 0D,1D &2D\n",
    "    - Binary cross entropy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression\n",
    "\n",
    "In this regresion type the input variable has multiple feature columns. \n",
    "For eg: in terms of employee salary prediction\n",
    "        input  -> age, name, years of experience, dept of work, designation\n",
    "        output -> salary for the employee\n",
    "        \n",
    "We will therefore see how the hypothesis function is deviced for such a multiple input linear regression probelem\n",
    "\n",
    "\n",
    "y = Xw + b\n",
    "\n",
    "The input vector should be x = 1xD\n",
    "\n",
    "weight/parameter = Dx1\n",
    "\n",
    "bias = 1\n",
    "    \n",
    "Now lets create an weight vector with bias using \"LINEAR module\" and then use the same to do the calculation for **Forward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Linear(in_features=2, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[-0.4651,  0.1902]])),\n",
       "             ('bias', tensor([-0.4801]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.],[3.,4.],[1.,-1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5647],\n",
       "        [-1.1145],\n",
       "        [-1.1354]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus Linear function is used to create the yhat output without explicitly coding the equation\n",
    "\n",
    "> yhat = b + wX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function for Multiple Linear Regression\n",
    "\n",
    "In this method we will build a multiple linear regression model by,\n",
    "\n",
    "- Creating a customer dataset using dataset class\n",
    "- Creating loss and optimizer object using exitsing packages\n",
    "- Training regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below class is a custom class inherited from \"Dataset\" class of torch.\n",
    "\n",
    "This class is basically to hold the dataset for training and methods to index call and length identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.zeros(20,2)\n",
    "        self.x[:,0] = torch.arange(-1,1,0.1)\n",
    "        self.x[:,1] = torch.arange(-1,1,0.1)\n",
    "        self.w = torch.tensor([[-10.],[-5.0]])\n",
    "        self.b = torch.tensor([-1.0])\n",
    "        \n",
    "        self.f = torch.mm(self.x,self.w) + self.b\n",
    "        self.y = self.f + (0.1*torch.randn((self.x.shape[0],1)))\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below class is an inherited class from nn.Module\n",
    "\n",
    "This class helps us in defining the weights and bias using linear class of torch and leveraging the same for forward pass calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lin_Reg(nn.Module):\n",
    "    def __init__(self,in_,out_):\n",
    "        super(Lin_Reg,self).__init__()\n",
    "        self.Linear = nn.Linear(in_,out_)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.Linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset()\n",
    "trainloader = DataLoader(dataset=data,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Linear.weight', tensor([[ 0.1538, -0.4462]])), ('Linear.bias', tensor([0.3360]))])\n"
     ]
    }
   ],
   "source": [
    "model = Lin_Reg(2,1)\n",
    "loss = nn.MSELoss()\n",
    "print(model.state_dict())\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets start training the model using the dataset defined for a batch size of 2 and epochs of 100 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model(x)\n",
    "        loss_ = loss(yhat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_.backward()       \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For multiple output regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = torch.zeros(20,2)\n",
    "        self.x[:,0] = torch.arange(-1,1,0.1)\n",
    "        self.x[:,1] = torch.arange(-1,1,0.1)\n",
    "        self.w = torch.tensor([[-10.],[-5.0]])\n",
    "        self.b = torch.tensor([-1.0])\n",
    "        \n",
    "        self.f = torch.mm(self.x,self.w) + self.b\n",
    "        self.y = torch.zeros(20,2)\n",
    "        self.y[:,0] = (self.f + (0.1*torch.randn((self.x.shape[0],1)))).view(20)\n",
    "        self.y[:,1] = (self.f + (0.1*torch.randn((self.x.shape[0],1)))).view(20)\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Linear.weight', tensor([[0.5148, 0.2370],\n",
      "        [0.2428, 0.2640]])), ('Linear.bias', tensor([ 0.2955, -0.0827]))])\n"
     ]
    }
   ],
   "source": [
    "data = dataset()\n",
    "trainloader = DataLoader(dataset=data,batch_size=2)\n",
    "\n",
    "#Specifing out variable to be 2 so that it takes corresponding y to fit the model and output 2 variables\n",
    "model = Lin_Reg(2,2)\n",
    "loss = nn.MSELoss()\n",
    "print(model.state_dict())\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model(x)\n",
    "        loss_ = loss(yhat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_.backward()       \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus this method will calculate forward pass and generate output for each of the target y respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression steps all applies here. Except that the plane that is formulated distingushes the classes in the target variable\n",
    "\n",
    "In terms of binary classification model, sigmoid function is passed on to the output z (wx+b) values to find the probability of class 0 and 1\n",
    "\n",
    "**Sigmoid -->  pushes the value of z between 0 and 1**\n",
    "\n",
    "to use sigmoid function use **torch.sigmoid()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Log_Reg(nn.Module):\n",
    "    def __init__(self,in_):\n",
    "        super(Log_Reg,self).__init__()\n",
    "        self.Linear = nn.Linear(in_,1)#in log reg the output param will always be one\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(self.Linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets take an input tensor with single scalar value (0 dimension)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of single scalar input is: tensor([0.5048], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Log_Reg(1)\n",
    "x = torch.tensor([1.0])\n",
    "\n",
    "yhat  = model.forward(x)\n",
    "print('The output of single scalar input is:',yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the output is between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets take a multiple sample input of 1D (one feature) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of single 1D input is: tensor([[0.5101],\n",
      "        [0.9743]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Log_Reg(1)\n",
    "x =  torch.tensor([[1.],[-5]])\n",
    "\n",
    "yhat  = model.forward(x)\n",
    "print('The output of single 1D input is:',yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally now we would take a multi sample 2D dimensional exmaple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of single 2D input is: tensor([[0.6109],\n",
      "        [0.3818]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Log_Reg(2)\n",
    "x =  torch.tensor([[1.,-1.],[-5,2.]])\n",
    "\n",
    "yhat  = model.forward(x)\n",
    "print('The output of single 2D input is:',yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase of logistic regression we use likelihood from bernouliie distribution\n",
    "\n",
    "where likelihod the product of probabilities of an outcome given the parameter values to **stay close to ground truth**\n",
    "\n",
    "so increasing the likelihood is the key to thsi process. Since log is a monotonically increaing function we use it to increase the likelihood\n",
    "\n",
    "With log the probabilities in product becomes additive in nature, thus -log will cause a concave function thus helping in reducing the loss while choose our parameters for best fit\n",
    "\n",
    "With the best parameter the lower point of the fucntion has **min loss and max likelihood**\n",
    "\n",
    "**Binary cross-entropy loss is give by:**\n",
    "\n",
    "j(w,x) = -1/n sumation(y log yhat + (1-y) log (1-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
